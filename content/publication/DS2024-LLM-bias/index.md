---
title: "Are Large Language Models Really Bias-Free? Jailbreak Prompts for Assessing Adversarial Robustness to Bias Elicitation"
date: 2024-08-12
publishDate: 2024-08-12
authors: ["Riccardo Cantini", "Giada Cosenza", "Alessio Orsino", "Domenico Talia"]
publication_types: ["1"]
abstract: "Large Language Models (LLMs) have revolutionized artificial intelligence, 
demonstrating remarkable computational power and linguistic capabilities. 
However, these models are inherently prone to various biases stemming from their training data. 
These include selection, linguistic, and confirmation biases, along with common stereotypes 
related to gender, ethnicity, sexual orientation, religion, socioeconomic status, disability, and age. 
This study explores the presence of these biases within the responses given by the most recent LLMs, 
analyzing the impact on their fairness and reliability. 
We also investigate how known prompt engineering techniques can be exploited to effectively reveal 
hidden biases of LLMs, testing their adversarial robustness against jailbreak prompts specially crafted for bias elicitation. Extensive experiments are conducted using the most widespread LLMs at different scales, confirming that LLMs can still be manipulated to produce biased or inappropriate responses, despite their advanced capabilities and sophisticated alignment processes. Our findings underscore the importance of enhancing mitigation techniques to address these safety issues, toward a more  sustainable and inclusive artificial intelligence."
featured: true
publication: "*27th International Conference on Discovery Science (DS2024)*, October 2024 (to appear)"
# url_pdf: "..."
# doi: "https://doi.org/10.48550/arXiv.2407.08441"
# Custom links:
links:
- name: Project
  url: https://github.com/rcantini/LLM-Bias-Jailbreak
  icon_pack: fab
  icon: github
# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder. 
image:
  caption: ""
  focal_point: ""
  preview_only: false

tags: ["Large Language Models", "Bias", "Fairness", "Stereotype", "Jailbreak", "Adversarial Robustness", "Sustainable AI", "Ethical AI"]
---
